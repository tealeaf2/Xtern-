{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Full Functions <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__ (self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.types_orders = [\"Fried Catfish\", \"Sugar Cream Pie\", \"Indiana Pork Chili\", \n",
    "                \"Indiana Corn Cob\", \"Buffalo Tacos\", \"Sweet Potato Fries\",\n",
    "                \"Grilled Cheese\", \"Pork Tenderloin Sandwich\", \"Cornbread Hushpuppies\",\n",
    "                \"BBQ Pulled Pork Sandwich\"]\n",
    "\n",
    "    def load_dataset(self, file):\n",
    "        df = pd.read_csv(file)\n",
    "        #print(df)\n",
    "        print(df.shape)\n",
    "\n",
    "        #Changes strings into integers that can be used for multiclass categorizing\n",
    "        for input in self.inputs:\n",
    "            df[input]=df[input].astype('category').cat.codes\n",
    "        df[self.outputs]=df[self.outputs].astype('category').cat.codes\n",
    "        print(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def train(self, df):\n",
    "        orders_X, orders_Y = df[self.inputs].values, df[self.outputs].values\n",
    "        # Split data 70% into train and 30% into test\n",
    "        x_orders_train, x_orders_test, y_orders_train, y_orders_test = train_test_split(orders_X, \n",
    "                                                                                        orders_Y, \n",
    "                                                                                        test_size = 0.30,\n",
    "                                                                                        random_state = 0,\n",
    "                                                                                        stratify=orders_Y)\n",
    "\n",
    "        columns = [0,1,2,3]\n",
    "        transformer = Pipeline(steps=[\n",
    "                        ('scaler', StandardScaler())\n",
    "                        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "                        transformers=[('preprocess', transformer, columns)])\n",
    "\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', SVC(probability=True))])\n",
    "        model = pipeline.fit(x_orders_train, y_orders_train)\n",
    "        predictions = model.predict(x_orders_test)\n",
    "\n",
    "        print (model)   \n",
    "        print('Predicted labels: ', predictions[:30])\n",
    "        print('Actual labels   : ' ,y_orders_test[:30])\n",
    "        \n",
    "        print(\"Overall Accuracy:\",accuracy_score(y_orders_test, predictions))\n",
    "        print(\"Overall Precision:\",precision_score(y_orders_test, predictions, average='weighted'))\n",
    "        return pickle.dumps(model)\n",
    "\n",
    "    def predict(self, model, inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        pred = model.predict(inputs)[0]\n",
    "        print(\"Prediction: \", self.types_orders[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Breakdown<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Process data function </h4>\n",
    "\n",
    "    def load_dataset(self, file):\n",
    "        df = pd.read_csv(file)\n",
    "        print(df)\n",
    "        print(df.shape)\n",
    "\n",
    "        for input in self.inputs:\n",
    "            df[input]=df[input].astype('category').cat.codes\n",
    "        df[self.outputs]=df[self.outputs].astype('category').cat.codes\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes in the file name and puts it into a dataframe. Since 4/5 of the variables are strings, I took each one of the columns and labeled each one of the inputs into a number with cat.codes. This way, the model could take inputs in as numbers and not strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Training <h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing what type of model I wanted, it had to be something that was able to take in multiple inputs and classify into a single output. At Sklearn's website, I found Multiclass Classification as my main model. All I had to do now was to decide what type of methods I would use to regularize the parameters. The one on the website had Logistic Regression, while another one I found had Singular Value Decomposition. So I made two models and chose the one to choose based on the higher accuracy. In the end, SVD had 55% accuracy while Logistic Regression only had 44%.\n",
    "\n",
    "        Code:\n",
    "        orders_X, orders_Y = df[self.inputs].values, df[self.outputs].values\n",
    "        # Split data 70% into train and 30% into test\n",
    "        x_orders_train, x_orders_test, y_orders_train, y_orders_test = train_test_split(orders_X, \n",
    "                                                                                        orders_Y, \n",
    "                                                                                        test_size = 0.30,\n",
    "                                                                                        random_state = 0,\n",
    "                                                                                        stratify=orders_Y)\n",
    "\n",
    "        columns = [0,1,2,3]\n",
    "        transformer = Pipeline(steps=[\n",
    "                        ('scaler', StandardScaler())\n",
    "                        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "                        transformers=[('preprocess', transformer, columns)])\n",
    "\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', SVC(probability=True))])\n",
    "        model = pipeline.fit(x_orders_train, y_orders_train)\n",
    "        predictions = model.predict(x_orders_test)\n",
    "\n",
    "        print (model)   \n",
    "        print('Predicted labels: ', predictions[:30])\n",
    "        print('Actual labels   : ' ,y_orders_test[:30])\n",
    "        \n",
    "        print(\"Overall Accuracy:\",accuracy_score(y_orders_test, predictions))\n",
    "        print(\"Overall Precision:\",precision_score(y_orders_test, predictions, average='weighted'))\n",
    "        return pickle.dumps(model)\n",
    "\n",
    "\n",
    "I first split the training and test data into a 70/30 split for training. Then, I scaled the columns of the data and put it into a pipline, where the model could fit into SVD. This reduced the parameters and made my model less overfit. Then, I printed out the results and returned the pickled model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running Code <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5)\n",
      "      Year  Major  University  Time  Order\n",
      "0        1     16           3     4      2\n",
      "1        2      5           0     6      7\n",
      "2        2      5           1     4      6\n",
      "3        1      3           3     3      2\n",
      "4        2      4           1     4      5\n",
      "...    ...    ...         ...   ...    ...\n",
      "4995     1     12           0     3      0\n",
      "4996     2      2           3     4      9\n",
      "4997     2      5           1     5      7\n",
      "4998     2      2           1     7      7\n",
      "4999     1      7           8     7      0\n",
      "\n",
      "[5000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "inputs = [\"Year\", \"Major\", \"University\", \"Time\"]\n",
    "output = \"Order\"\n",
    "file = \"Xtern_traindata.csv\"\n",
    "\n",
    "model = Model(inputs, output)\n",
    "df= model.load_dataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('preprocess',\n",
      "                                                  Pipeline(steps=[('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  [0, 1, 2, 3])])),\n",
      "                ('regressor', SVC(probability=True))])\n",
      "Predicted labels:  [2 9 7 0 7 7 3 5 7 6 5 5 7 3 5 5 7 0 7 3 5 4 8 0 5 7 5 5 8 7]\n",
      "Actual labels   :  [2 9 7 0 7 7 3 0 7 6 5 5 7 1 5 5 7 0 9 8 6 4 9 0 1 5 4 5 8 7]\n",
      "Overall Accuracy: 0.5573333333333333\n",
      "Overall Precision: 0.5668067664703302\n"
     ]
    }
   ],
   "source": [
    "trained_pickled_model = model.train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  Grilled Cheese\n"
     ]
    }
   ],
   "source": [
    "#Inputs: Year 2, Astronomy, Bulter University, 12\n",
    "inputs = [[1, 2, 1, 4]]\n",
    "model.predict(pickle.loads(trained_pickled_model), inputs)\n",
    "\n",
    "#Note: No idea if this is right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Considerations<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is suitable, then I would test this model a lot more, repeatedly checking the predictions of the model to people's actual orders. Depending on it's success, then I would deploy the model for actual use. The model choise would be another consideration, as the one I've coded here is only 55% successful. If I could've found a better model or used a different method to train the model, there could've been a higher accuracy. The last thing I would consider is the scaling of my values. Because this model is meant to predict what someone would order, when I scaled my values, it might have overgeneralized too much and made my accuracy lower than it should've been."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
